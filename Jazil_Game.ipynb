{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import datasets\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = os.getenv(\"IBM_WATSONX_API_KEY\")\n",
    "PROJECT_ID = os.getenv(\"IBM_WATSONX_PROJECT_ID\")\n",
    "API_URL = os.getenv(\"IBM_WATSONX_URL\")\n",
    "MODEL_ID = \"sdaia/allam-1-13b-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the embedding function\n",
    "emb_func = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_access_token():\n",
    "    token_url = \"https://iam.cloud.ibm.com/identity/token\"\n",
    "    headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n",
    "    data = {\n",
    "        \"grant_type\": \"urn:ibm:params:oauth:grant-type:apikey\",\n",
    "        \"apikey\": API_KEY\n",
    "    }\n",
    "    response = requests.post(token_url, headers=headers, data=data)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"access_token\"]\n",
    "    else:\n",
    "        raise Exception(f\"Error retrieving token: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_data(num_rows=100000):\n",
    "    # Load the Ashaar dataset from Hugging Face\n",
    "    dataset = datasets.load_dataset('arbml/ashaar')\n",
    "    \n",
    "    # Load only the first `num_rows` rows from the 'train' split\n",
    "    df = pd.DataFrame(dataset['train'].select(range(num_rows)))  # 'train' is the split name; use 'test' if needed\n",
    "    \n",
    "    # Ensure all necessary columns are available and of type string\n",
    "    df = df.astype(str).dropna(subset=['poem title', 'poem meter', 'poem verses', 'poem theme', \n",
    "                                       'poem url', 'poet name', 'poet description', \n",
    "                                       'poet url', 'poet era'])\n",
    "    \n",
    "    # Combine relevant columns into a single column for the verse\n",
    "    df['text'] = df[['poem title', 'poem meter', 'poem verses', 'poem theme', \n",
    "                    'poem url', 'poet name', 'poet description', \n",
    "                    'poet url', 'poet era']].agg(' '.join, axis=1)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(df):\n",
    "    embeddings_file = 'poem_embeddings.npy'\n",
    "    if os.path.exists(embeddings_file):\n",
    "        print(\"Loading existing embeddings...\")\n",
    "        return np.load(embeddings_file)\n",
    "    else:\n",
    "        print(\"Creating new embeddings...\")\n",
    "        embeddings = emb_func.embed_documents(df['text'].tolist())\n",
    "        np.save(embeddings_file, embeddings)\n",
    "        return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to embed AI-generated verse\n",
    "def create_embedding_for_ai_verse(verse):\n",
    "    # Use the same embedding function for AI-generated verse to maintain consistency\n",
    "    return emb_func.embed_documents([verse])[0]  # Embedding a single verse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_letter(letter):\n",
    "    return 'ا' if letter in ['أ', 'إ', 'ء', 'ى','ئ'] else letter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def starts_with_letter(verse, letter):\n",
    "    if letter is None:\n",
    "        return True  # Allow any letter if last_letter is None\n",
    "    verse = re.sub(r'[\\u064B-\\u0652]', '', verse)\n",
    "    letter = re.sub(r'[\\u064B-\\u0652]', '', letter)\n",
    "    return normalize_letter(verse[0]) == normalize_letter(letter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_similar_verses(query, last_letter, embeddings, df, top_k=10):\n",
    "    query_embedding = emb_func.embed_query(query)\n",
    "    similarities = cosine_similarity([query_embedding], embeddings)[0]\n",
    "    \n",
    "    mask = df['poem verses'].apply(lambda x: starts_with_letter(x, last_letter))\n",
    "    filtered_similarities = similarities * mask\n",
    "    \n",
    "    top_indices = np.argsort(filtered_similarities)[-top_k:][::-1]\n",
    "    return df.iloc[top_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_letter(verse):\n",
    "    # Remove diacritics and non-word characters\n",
    "    verse = re.sub(r'[ًٌٍَُِّْـ\\W]', '', verse)\n",
    "    # List of possible elongations\n",
    "    elongations = ['ا', 'و', 'ي', 'ها', 'ما', 'با', 'سا', 'دا', 'غا', 'فا', 'طا', 'جا', 'زا', 'شا', 'عا', 'قا', 'لا', 'نا', 'كا']\n",
    "    # List of possible endings to ignore\n",
    "    endings_to_ignore = ['ة', 'ه', 'هم', 'هن', 'هما','وا']\n",
    "    \n",
    "    if not verse:\n",
    "        return None\n",
    "    \n",
    "    # Check if the verse ends with an elongation\n",
    "    for elongation in elongations:\n",
    "        if verse.endswith(elongation):\n",
    "            return verse[-len(elongation)-1] if len(verse) > len(elongation) else verse[0]\n",
    "    \n",
    "    # Check if the verse ends with any of the endings to ignore\n",
    "    for ending in endings_to_ignore:\n",
    "        if verse.endswith(ending):\n",
    "            return verse[-len(ending)-1]\n",
    "    \n",
    "    return verse[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt, access_token):\n",
    "    url = \"https://eu-de.ml.cloud.ibm.com/ml/v1/text/generation?version=2023-05-29\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {access_token}\",\n",
    "    }\n",
    "    data = {\n",
    "        \"input\": prompt,\n",
    "        \"parameters\": {\n",
    "\t\t\"decoding_method\": \"greedy\",\n",
    "\t\t\"max_new_tokens\": 600,\n",
    "\t\t\"min_new_tokens\": 0,\n",
    "\t\t\"stop_sequences\": [],\n",
    "\t\t\"repetition_penalty\": 1\n",
    "\t},\n",
    "        \"model_id\": MODEL_ID,\n",
    "        \"project_id\": PROJECT_ID\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()['results'][0]['generated_text']\n",
    "    else:\n",
    "        print(f\"Error generating response from Allam model: {response.text}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity calculation between two vectors\n",
    "def calculate_cosine_similarity(vector1, vector2):\n",
    "    # Reshape vectors if necessary\n",
    "    vector1 = vector1.reshape(1, -1)\n",
    "    vector2 = vector2.reshape(1, -1)\n",
    "    # Calculate cosine similarity\n",
    "    similarity = cosine_similarity(vector1, vector2)[0][0]\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_with_dataset(ai_generated_verse, dataset_embeddings):\n",
    "    # Embed the AI-generated verse\n",
    "    ai_embedding = emb_func.embed_documents([ai_generated_verse])  # This returns a list\n",
    "    \n",
    "    # Convert ai_embedding to a numpy array\n",
    "    ai_embedding = np.array(ai_embedding)\n",
    "    \n",
    "    # Ensure ai_embedding is 2D\n",
    "    if ai_embedding.ndim == 1:\n",
    "        ai_embedding = ai_embedding.reshape(1, -1)\n",
    "    \n",
    "    # Check that the dimensions match between AI embedding and dataset embeddings\n",
    "    if ai_embedding.shape[1] != dataset_embeddings.shape[1]:\n",
    "        raise ValueError(f\"Incompatible dimensions: AI embedding has {ai_embedding.shape[1]} dimensions, while dataset embeddings have {dataset_embeddings.shape[1]} dimensions.\")\n",
    "\n",
    "    # Now proceed with cosine similarity comparison\n",
    "    max_similarity = -1\n",
    "    most_similar_verse = None\n",
    "\n",
    "    for verse_embedding in dataset_embeddings:\n",
    "        similarity = calculate_cosine_similarity(ai_embedding, verse_embedding)\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "\n",
    "    return max_similarity, most_similar_verse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing embeddings...\n",
      "Welcome to Jazil! Let's start the poetic exchange.\n",
      "اختر مستوى الصعوبة:\n",
      "1. سهل: يجب عليك كتابة 6 أبيات.\n",
      "2. صعب: يجب عليك كتابة 10 أبيات.\n",
      "لقد اخترت المستوى السهل.\n",
      "شكرًا للعب في جزيل!\n",
      "\n",
      "تقييم أبيات الذكاء الاصطناعي:\n",
      "دقة الذكاء الاصطناعي: 0/0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "def play_jazil():\n",
    "    df = load_and_clean_data()\n",
    "    embeddings = create_embeddings(df)\n",
    "    access_token = get_access_token()\n",
    "    dataset_embeddings = np.array(embeddings)\n",
    "    \n",
    "    print(\"Welcome to Jazil! Let's start the poetic exchange.\")\n",
    "    print(\"اختر مستوى الصعوبة:\")\n",
    "    print(\"1. سهل: يجب عليك كتابة 6 أبيات.\")\n",
    "    print(\"2. صعب: يجب عليك كتابة 10 أبيات.\")\n",
    "    \n",
    "    mode = input(\"اختر رقم المستوى: \")\n",
    "    if mode == '1':\n",
    "        required_verses = 6\n",
    "        print(\"لقد اخترت المستوى السهل.\")\n",
    "    elif mode == '2':\n",
    "        required_verses = 10\n",
    "        print(\"لقد اخترت المستوى الصعب.\")\n",
    "    else:\n",
    "        print(\"اختيار غير صالح. سيتم اختيار المستوى السهل بشكل افتراضي.\")\n",
    "        required_verses = 6\n",
    "\n",
    "    last_letter = None\n",
    "    ai_turn = False\n",
    "    used_verses = set()\n",
    "    conversation_history = []\n",
    "    ai_verses = []  # Store AI-generated verses for later evaluation\n",
    "    user_verses_count = 0  # Count the number of valid verses from the user\n",
    "    \n",
    "    while True:\n",
    "        if user_verses_count >= required_verses:\n",
    "            print(f\"تهانينا! لقد ربحت بعد كتابة {user_verses_count} أبيات.\")\n",
    "            break\n",
    "        \n",
    "        if not ai_turn:\n",
    "            user_input = input(\"دورك (أو اكتب 'quit' للخروج): \")\n",
    "            if user_input.lower() == 'quit':\n",
    "                print(\"شكرًا للعب في جزيل!\")\n",
    "                break\n",
    "            \n",
    "            if user_input in used_verses:\n",
    "                print(\"هذا البيت قد تم استخدامه بالفعل. يرجى تقديم بيت مختلف.\")\n",
    "                continue\n",
    "\n",
    "            if last_letter and not starts_with_letter(user_input, last_letter):\n",
    "                print(f\"يجب أن يبدأ بيتك بالحرف '{last_letter}'. حاول مرة أخرى.\")\n",
    "                continue\n",
    "\n",
    "            # Verify the verse using RAG\n",
    "            similar_verses = retrieve_similar_verses(user_input, last_letter, embeddings, df)\n",
    "            context = \"\\n\".join(similar_verses['poem verses'].tolist())\n",
    "            \n",
    "            prompt = f\"\"\"\n",
    "            أنت خبير متخصص في الشعر العربي الكلاسيكي. مهمتك هي تحليل النص المقدم وتحديد ما إذا كان بيتًا شعريًا صحيحًا أم لا.\n",
    "            قم بتقييم النص التالي بدقة شديدة وفقًا للمعايير الآتية:\n",
    "            1. الشكل: هل يتكون النص من صدر وعجز متوازنين؟\n",
    "            2. الوزن الشعري: هل يتبع النص أحد البحور الشعرية العربية المعروفة؟\n",
    "            3. القافية: هل توجد قافية واضحة ومناسبة في نهاية البيت؟\n",
    "            4. اللغة: هل يستخدم النص لغة عربية فصحى وتراكيب شعرية تقليدية؟\n",
    "            5. المعنى: هل للنص معنى شعري واضح ومتماسك؟\n",
    "            6. الأصالة: هل يبدو النص كأنه من الشعر العربي الكلاسيكي وليس مجرد جملة عادية أو نصًا حديثًا؟\n",
    "            7. هل البيت المذكور يحتوي فقط حرف؟             \n",
    "            8. هل البيت المذكور يحتوي على قافية ومعنى شعري ؟\n",
    "            النص المراد تقييمه:\n",
    "            {user_input}\n",
    "\n",
    "            أمثلة على أبيات شعرية مشابهة:\n",
    "            {context}\n",
    "            يجب ان يكون البيت المذكور يتناسب مع الامثلة المذكورة اعلاه\n",
    "\n",
    "            بعد التحليل الدقيق، قدم إجابة مفصلة:\n",
    "            1. صنف النص إما كـ \"بيت شعري صحيح\" أو \"ليس بيتًا شعريًا صحيحًا\".\n",
    "            \"\"\"\n",
    "            verification_response = generate_response(prompt, access_token)\n",
    "\n",
    "            if \"بيت شعري صحيح\" in verification_response.lower():\n",
    "                print(f\"المستخدم: {user_input}\")\n",
    "                used_verses.add(user_input)\n",
    "                last_letter = get_last_letter(user_input)\n",
    "                conversation_history.append(f\"User: {user_input}\")\n",
    "                user_verses_count += 1  # Increment user's verse count\n",
    "                ai_turn = True\n",
    "            else:\n",
    "                print(\"هذا النص ليس بيتًا شعريًا صحيحًا. يرجى تقديم بيت شعري حقيقي.\")\n",
    "                print(\"سبب الرفض:\", verification_response)\n",
    "                continue\n",
    "\n",
    "        else:\n",
    "            print(\"دور الذكاء الاصطناعي...\")\n",
    "            \n",
    "            similar_verses = retrieve_similar_verses(\" \".join(conversation_history[-3:]), last_letter, embeddings, df)\n",
    "            context = \"\\n\".join(similar_verses['poem verses'].tolist())\n",
    "            prompt = f\"\"\"\n",
    "            أنت شاعر عربي ماهر. مهمتك هي إنتاج بيت شعري واحد فقط يتناسب مع سياق المحادثة الحالية ويبدأ بالحرف المطلوب.\n",
    "\n",
    "            سياق المحادثة:\n",
    "            {' '.join(conversation_history[-3:])}\n",
    "\n",
    "            الحرف الذي يجب أن يبدأ به البيت الجديد: {last_letter if last_letter else 'أي حرف'}\n",
    "\n",
    "            أمثلة على أبيات شعرية مشابهة:\n",
    "            {context}\n",
    "            خذ بيت شعري واحد فقط من هذه الأمثلة.\n",
    "            قم بإنتاج بيت شعري جديد يتبع القواعد التالية:\n",
    "            1. يبدأ بالحرف المحدد (إذا كان محددًا).\n",
    "            2. يتكون من صدر وعجز متوازنين، مفصولين بسطر جديد.\n",
    "            3. يتبع أحد البحور الشعرية العربية المعروفة.\n",
    "            4. يحتوي على قافية مناسبة.\n",
    "            5. يستخدم لغة عربية فصحى وتراكيب شعرية تقليدية.\n",
    "            6. له معنى شعري واضح ومتماسك.\n",
    "            7. يبدو كجزء أصيل من الشعر العربي الكلاسيكي.\n",
    "\n",
    "            ملاحظة مهمة: أنتج بيت شعري واحد فقط، لا تضف أي شرح أو تعليق. اكتب الصدر في سطر والعجز في السطر التالي.\n",
    "            لاتكتب حرف واحد وتظن انه بيت شعري هذا خطا انتج بيت شعري واضح ومتماسك\n",
    "            أنتج البيت الشعري:\n",
    "            \"\"\"\n",
    "\n",
    "            generated_text = generate_response(prompt, access_token)\n",
    "\n",
    "            if generated_text and generated_text not in used_verses:\n",
    "                print(f\"الذكاء الاصطناعي: {generated_text}\")\n",
    "                used_verses.add(generated_text)\n",
    "                last_letter = get_last_letter(generated_text)  # Get last letter from the second line (عجز)\n",
    "                conversation_history.append(f\"AI: {generated_text}\")\n",
    "                ai_verses.append(generated_text)  \n",
    "                ai_turn = False\n",
    "            else:\n",
    "                print(\"لم أستطع إنتاج بيت شعري مناسب. دورك.\")\n",
    "                ai_verses.append(generated_text)  # Store AI verse for later evaluation\n",
    "                ai_turn = False\n",
    "            # Perform evaluation at the end\n",
    "    evaluate_ai_verses(ai_verses, dataset_embeddings)\n",
    "\n",
    "def evaluate_ai_verses(ai_verses, dataset_embeddings):\n",
    "    print(\"\\nتقييم أبيات الذكاء الاصطناعي:\")\n",
    "    accurate_ai_verses = 0\n",
    "    total_ai_verses = len(ai_verses)\n",
    "    similarity_threshold = 0.65  # Adjust as needed\n",
    "\n",
    "    for verse in ai_verses:\n",
    "        max_similarity, _ = compare_with_dataset(verse, dataset_embeddings)\n",
    "        if max_similarity >= similarity_threshold:\n",
    "            accurate_ai_verses += 1\n",
    "\n",
    "    accuracy = (accurate_ai_verses / total_ai_verses) * 100 if total_ai_verses > 0 else 0\n",
    "    print(f\"دقة الذكاء الاصطناعي: {accurate_ai_verses}/{total_ai_verses} ({accuracy:.2f}%)\")\n",
    "\n",
    "# Run the game\n",
    "play_jazil()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
